{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Human Activity Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "18lPKjrwSnZvxAFiSW5VBN49dFuTeTxHj",
      "authorship_tag": "ABX9TyNBU+Pb5MA6ZPnfKqjqYTUZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ItShivani/Human-Activity-Recognition/blob/master/Human_Activity_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Dehleahdyc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "from collections import deque\n",
        "import sys\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69Zky3TSeaEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CLASSES = open(\"/content/drive/My Drive/human-activity-recognition/action_recognition_kinetics.txt\").readlines()\n",
        "SAMPLE_DURATION = 16\n",
        "SAMPLE_SIZE = 112"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vy10UYCAUpM1",
        "colab_type": "code",
        "outputId": "a8d77620-bada-493f-8564-f5078fee5e66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "frames = deque(maxlen=SAMPLE_DURATION)\n",
        "# load the human activity recognition model\n",
        "print(\"[INFO] loading human activity recognition model...\")\n",
        "net = cv2.dnn.readNet(\"/content/drive/My Drive/human-activity-recognition/resnet-34_kinetics.onnx\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading human activity recognition model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVf_7xRhVC6M",
        "colab_type": "code",
        "outputId": "02ce6850-1db0-4da7-8263-49990dc73b42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(\"[INFO] accessing video stream...\")\n",
        "vs = cv2.VideoCapture(\"/content/drive/My Drive/human-activity-recognition/example_activities.mp4\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] accessing video stream...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9xCj0DeVr1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loop over frames from the video stream\n",
        "while True:\n",
        "\t# read a frame from the video stream\n",
        "  (grabbed, frame) = vs.read()\n",
        "\n",
        "  # if the frame was not grabbed then we've reached the end of\n",
        "  # the video stream so break from the loop\n",
        "  if not grabbed:\n",
        "    print(\"[INFO] no frame read from stream - exiting\")\n",
        "    break\n",
        "\n",
        "  # resize the frame (to ensure faster processing) and add the\n",
        "  # frame to our queue\n",
        "  frame = imutils.resize(frame, width=400)\n",
        "  frames.append(frame)\n",
        "\n",
        "  # if our queue is not filled to the sample size, continue back to\n",
        "  # the top of the loop and continue polling/processing frames\n",
        "  if len(frames) < SAMPLE_DURATION:\n",
        "    continue\n",
        "\n",
        "\t# now that our frames array is filled we can construct our blob\n",
        "  blob = cv2.dnn.blobFromImages(frames, 1.0,(SAMPLE_SIZE, SAMPLE_SIZE), (114.7748, 107.7354, 99.4750),swapRB=True, crop=True)\n",
        "  blob = np.transpose(blob, (1, 0, 2, 3))\n",
        "  blob = np.expand_dims(blob, axis=0)\n",
        "\n",
        "  # pass the blob through the network to obtain our human activity\n",
        "  # recognition predictions\n",
        "  net.setInput(blob)\n",
        "  outputs = net.forward()\n",
        "  label = CLASSES[np.argmax(outputs)]\n",
        "  label = label[:-1]\n",
        "\n",
        "  # draw the predicted activity on the frame\n",
        "  cv2.rectangle(frame, (0, 0), (300, 40), (0, 0, 0), -1)\n",
        "  cv2.putText(frame, label, (10, 25), cv2.FONT_HERSHEY_SIMPLEX,0.8, (255, 255, 255), 2)\n",
        "\n",
        "\t# display the frame to our screen\n",
        "\t#cv2.imshow(\"Activity Recognition\", frame)\n",
        "  cv2_imshow(frame)\n",
        "  \n",
        "  key = cv2.waitKey(1) & 0xFF\n",
        "\n",
        "\t# if the `q` key was pressed, break from the loop\n",
        "  if key == ord(\"q\"):\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtQc0T27V6MI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}